{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "useful-chaos",
   "metadata": {},
   "source": [
    "### Feature (text-embeddings) extraction from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import modeling\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'  ## uncased BERT base model\n",
    "BERT_PRETRAINED_DIR = './Akshay/Desktop/DataWeave/Experiments/' + BERT_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, layer_indexes, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    input_type_ids = features[\"input_type_ids\"]\n",
    "\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=False,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=input_type_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        exit\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    scaffold_fn = None\n",
    "    (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
    "         tvars, init_checkpoint)\n",
    "    \n",
    "    if use_tpu:\n",
    "        def tpu_scaffold():\n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "            return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "    else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    for var in tvars:\n",
    "        init_string = \"\"\n",
    "        if var.name in initialized_variable_names:\n",
    "            tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    all_layers = model.get_all_encoder_layers()\n",
    "\n",
    "    predictions = {\n",
    "        \"unique_id\": unique_ids,\n",
    "    }\n",
    "\n",
    "    for (i, layer_index) in enumerate(layer_indexes):\n",
    "        predictions[\"layer_output_%d\" % i] = all_layers[layer_index]\n",
    "\n",
    "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, seq_length, tokenizer):\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "    else:\n",
    "        if len(tokens_a) > seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    input_type_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    input_type_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        input_type_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_type_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "    if ex_index < 5:\n",
    "        [tokenization.printable_text(x) for x in tokens]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "            unique_id=example.unique_id,\n",
    "            tokens=tokens,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            input_type_ids=input_type_ids))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sequence(input_sentences):\n",
    "    sentences = []\n",
    "    unique_id = 0\n",
    "    for sentence in input_sentences:\n",
    "        line = tokenization.convert_to_unicode(sentence)\n",
    "        sentences.append(InputExample(unique_id=unique_id, text_a=line))\n",
    "    unique_id += 1\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-pavilion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-monitoring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "monthly-cooling",
   "metadata": {},
   "source": [
    "### Sentence BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\.conda\\envs\\pytorchenv\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "## Other available models\n",
    "# all-roberta-large-v1\n",
    "# all-distilroberta-v1 \n",
    "# all-MiniLM-L6-v2\n",
    "# distiluse-base-multilingual-cased-v1\n",
    "# paraphrase-TinyBERT-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organized-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['A test sentences for checking',\n",
    "    'Amazon and Flipkart are two major ecomm merchants in India', \n",
    "    'One plus nord mobile with 5000 mAH battery on offer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rolled-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "taken-newton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the embeddings\n",
    "\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "figured-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "reliable-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = model.encode(\"Salt Water Sandals by Hoy Original Sandal (Baby, Walker, Toddler, Little Kid & Big Kid) SHINY YELLOW Little Kid 2 M\")\n",
    "emb2 = model.encode(\"Salt Water Sandals by Hoy Shoe The Original Sandal Shiny Yellow 1 Little Kid  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "documented-hospital",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity:  0.9377158\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos = np.dot(emb1, emb2)\n",
    "cos = cos / (np.linalg.norm(emb1)* np.linalg.norm(emb2))\n",
    "print(\"Cosine similarity: \", cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "spatial-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product:  tensor([[0.9377]])\n"
     ]
    }
   ],
   "source": [
    "dot_prod = util.dot_score(emb1, emb2)\n",
    "print(\"Dot product: \", dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "collective-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = model.encode(\"I bought an Apple PC on sale\")\n",
    "t2 = model.encode(\"I bought an Apple fruit on sale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "facial-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6631]])\n"
     ]
    }
   ],
   "source": [
    "cos_sim = util.cos_sim(t1, t2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-adelaide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "detailed-toner",
   "metadata": {},
   "source": [
    "### comparision with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "unique-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "numerical-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = nlp(\"Salt Water Sandals by Hoy Original Sandal (Baby, Walker, Toddler, Little Kid & Big Kid) SHINY YELLOW Little Kid 2 M\")\n",
    "text2 = nlp(\"Salt Water Sandals by Hoy Shoe The Original Sandal Shiny Yellow 1 Little Kid  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "animal-harvest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9035214342821734\n"
     ]
    }
   ],
   "source": [
    "print(text1.similarity(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "brutal-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941827671682806\n"
     ]
    }
   ],
   "source": [
    "text1 = nlp(\"I bought an Apple PC on sale\")\n",
    "text2 = nlp(\"I bought an Apple fruit on sale\")\n",
    "\n",
    "print(text1.similarity(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "freelance-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salt Water Sandals ORG \tstart_pos 0 end_pos 18\n",
      "Hoy Original Sandal ORG \tstart_pos 22 end_pos 41\n",
      "Walker PERSON \tstart_pos 49 end_pos 55\n",
      "Toddler ORG \tstart_pos 57 end_pos 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\.conda\\envs\\pytorchenv\\lib\\runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Salt Water Sandals\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hoy Original Sandal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (Baby, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Walker\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Toddler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", Little Kid &amp; Big Kid) SHINY YELLOW Little Kid 2 M</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "for ent in text1.ents:\n",
    "    print(ent.text,  ent.label_, \"\\tstart_pos\" , ent.start_char, \"end_pos\", ent.end_char)\n",
    "    #tokens.append(text1[ent.start_char:ent.end_char])\n",
    "    \n",
    "displacy.serve(text1, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-inside",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-enclosure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-valuation",
   "metadata": {},
   "source": [
    "#### top pairwise matches in case of multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity bet all pairs\n",
    "\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all pairs of list with their cosine similarity\n",
    "\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the list by highest cosine similarity\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3  ## top 'n' matches \n",
    "    \n",
    "print(\"Top {} most similar pairs: \".format(n))\n",
    "\n",
    "for score, i, j in all_sentence_combinations[0:n]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
