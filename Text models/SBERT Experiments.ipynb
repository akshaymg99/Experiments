{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scenic-front",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\.conda\\envs\\pytorchenv\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "## installation: https://github.com/UKPLab/sentence-transformers\n",
    "# https://www.sbert.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "western-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "## Other available models: https://www.sbert.net/docs/pretrained_models.html\n",
    "# all-roberta-large-v1\n",
    "# all-distilroberta-v1 \n",
    "# all-MiniLM-L6-v2\n",
    "# distiluse-base-multilingual-cased-v1\n",
    "# paraphrase-TinyBERT-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "confused-mailman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-italian",
   "metadata": {},
   "source": [
    "##### sentence similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "covered-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = model.encode(\"Jo Malone London™ Dark Amber & Ginger Lily Scented Home Candle NO COLOR  7 oz—\")\n",
    "emb2 = model.encode(\"Jo Malone London Dark Amber & Ginger Lily Home Candle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "enormous-military",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity:  0.92585653\n"
     ]
    }
   ],
   "source": [
    "cos = np.dot(emb1, emb2)\n",
    "cos = cos / (np.linalg.norm(emb1)* np.linalg.norm(emb2))\n",
    "print(\"Cosine similarity: \", cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "comparable-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.9259]])\n"
     ]
    }
   ],
   "source": [
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-constitutional",
   "metadata": {},
   "source": [
    "##### word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "third-tooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.7672]])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.encode(\"jacket\")\n",
    "w2 = model.encode(\"raincoat\")\n",
    "\n",
    "cos_sim = util.cos_sim(w1, w2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-flavor",
   "metadata": {},
   "source": [
    "##### alpha-numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "parliamentary-appeal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.9193]])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.encode(\"3 ½ Pillow\")\n",
    "w2 = model.encode(\"3.5 Pillow\")\n",
    "\n",
    "cos_sim = util.cos_sim(w1, w2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-reggae",
   "metadata": {},
   "source": [
    "###### spell-mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "framed-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.7792]])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.encode(\"weterproof\")\n",
    "w2 = model.encode(\"water proof\")\n",
    "\n",
    "cos_sim = util.cos_sim(w1, w2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-burden",
   "metadata": {},
   "source": [
    "###### prefix / suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "official-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.6858]])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.encode(\"clean\")\n",
    "w2 = model.encode(\"cleaner\")\n",
    "\n",
    "cos_sim = util.cos_sim(w1, w2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-recognition",
   "metadata": {},
   "source": [
    "###### singular / plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "protecting-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.8166]])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.encode(\"shoe\")\n",
    "w2 = model.encode(\"shoes\")\n",
    "\n",
    "cos_sim = util.cos_sim(w1, w2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-independence",
   "metadata": {},
   "source": [
    "##### Context check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "manufactured-middle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.5274]])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.encode(\"River bank holds surprises for people\")\n",
    "w2 = model.encode(\"‘Federal bank holds surprise for people\")\n",
    "\n",
    "cos_sim = util.cos_sim(w1, w2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-spiritual",
   "metadata": {},
   "source": [
    "##### Phrase detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "future-classroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.4101]])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.encode(\"water proof\")\n",
    "w2 = model.encode(\"resistant\")\n",
    "\n",
    "cos_sim = util.cos_sim(w1, w2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-graham",
   "metadata": {},
   "source": [
    "##### plotting histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "golden-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('processed_Dtrain_4f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "balanced-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for row in corpus['title']:\n",
    "    titles.append((row))\n",
    "    \n",
    "match_titles = []\n",
    "for row in corpus['match_title']:\n",
    "    match_titles.append((row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "innovative-franchise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308041"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "asian-grocery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(len(titles) - 208041):\n",
    "    s1 = titles[i]\n",
    "    s2 = match_titles[i]\n",
    "    \n",
    "    if s1 is np.NaN or s2 is np.NaN:\n",
    "        continue\n",
    "    if len(s1) == 0 or len(s2) == 0:\n",
    "        continue\n",
    "    \n",
    "    w1 = model.encode(s1)\n",
    "    w2 = model.encode(s2)\n",
    "    cos_sim = util.cos_sim(w1, w2)\n",
    "    if cos_sim > 0:\n",
    "        scores.append(cos_sim)\n",
    "    if (i % 10000 == 0):\n",
    "        print(i/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "functioning-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(scores)\n",
    "dataset = pd.DataFrame(scores)\n",
    "dataset.to_csv('SBERT_allmpnet_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-reduction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-division",
   "metadata": {},
   "source": [
    "### Plotting other models histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "yellow-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cross-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('./glove.42B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]           ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32')   ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-locator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-storage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fitting-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "public-today",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\.conda\\envs\\pytorchenv\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n",
      "35.0\n",
      "36.0\n",
      "37.0\n",
      "38.0\n",
      "39.0\n",
      "40.0\n",
      "41.0\n",
      "42.0\n",
      "43.0\n",
      "44.0\n",
      "45.0\n",
      "46.0\n",
      "47.0\n",
      "48.0\n",
      "49.0\n",
      "50.0\n",
      "51.0\n",
      "52.0\n",
      "53.0\n",
      "54.0\n",
      "55.0\n",
      "56.0\n",
      "57.0\n",
      "58.0\n",
      "59.0\n",
      "60.0\n",
      "61.0\n",
      "62.0\n",
      "63.0\n",
      "64.0\n",
      "65.0\n",
      "66.0\n",
      "67.0\n",
      "68.0\n",
      "69.0\n",
      "70.0\n",
      "71.0\n",
      "72.0\n",
      "73.0\n",
      "74.0\n",
      "75.0\n",
      "76.0\n",
      "77.0\n",
      "78.0\n",
      "79.0\n",
      "80.0\n",
      "81.0\n",
      "82.0\n",
      "83.0\n",
      "84.0\n",
      "85.0\n",
      "86.0\n",
      "87.0\n",
      "88.0\n",
      "89.0\n",
      "90.0\n",
      "91.0\n",
      "92.0\n",
      "93.0\n",
      "94.0\n",
      "95.0\n",
      "96.0\n",
      "97.0\n",
      "98.0\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(len(titles) - 208041):\n",
    "    s1 = titles[i]\n",
    "    s2 = match_titles[i]\n",
    "    \n",
    "    if s1 is np.NaN or s2 is np.NaN:\n",
    "        continue\n",
    "    if len(s1) == 0 or len(s2) == 0:\n",
    "        continue\n",
    "    \n",
    "    e1 = sen_emb(s1) \n",
    "    e2 = sen_emb(s2)\n",
    "    e1, e2 = pad_emb(e1, e2, DIM)\n",
    "    cos_sim = dot(e1, e2)/(norm(e1)*norm(e2))\n",
    "    \n",
    "    if cos_sim > 0:\n",
    "        scores.append(cos_sim)\n",
    "    if (i % 1000 == 0):\n",
    "        print(i/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "royal-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(scores)\n",
    "dataset = pd.DataFrame(scores)\n",
    "dataset.to_csv('scores_glove_42B_300d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-program",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-wildlife",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "blond-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "PUNCT_TO_REMOVE = PUNCT_TO_REMOVE.replace(\"-\",\"\")\n",
    "PUNCT_TO_REMOVE = PUNCT_TO_REMOVE.replace(\".\",\"\")\n",
    "PUNCT_TO_REMOVE = PUNCT_TO_REMOVE.replace(\"&\",\"\")\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "\n",
    "def sen_emb(sen):\n",
    "    # pre-process the sentence\n",
    "    sen = sen.replace('-',' ')\n",
    "    sen = sen.replace('/',' ')\n",
    "    sen = remove_special_chars(sen)\n",
    "    sen = sen.replace('&','and')\n",
    "    sen = sen.lower()\n",
    "    \n",
    "    ## split the sen to list of words\n",
    "    words = sen.split()\n",
    "    \n",
    "    emb = []\n",
    "    emb = np.array(emb)\n",
    "        \n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec = embeddings_index[word]\n",
    "        \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "        emb = np.concatenate([emb, vec])\n",
    "        \n",
    "    return emb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "sublime-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_emb(e1, e2, dim):\n",
    "    s_ = e1.shape\n",
    "    size_1 = s_[0]\n",
    "    \n",
    "    s_ = e2.shape\n",
    "    size_2 = s_[0]\n",
    "    \n",
    "    if size_1 == size_2:\n",
    "        return [e1, e2]\n",
    "    \n",
    "    elif size_1 < size_2:\n",
    "        for _ in range(size_2 - size_1):\n",
    "            e1 = np.append(e1, 0.0)\n",
    "        \n",
    "    elif size_1 > size_2:\n",
    "        for _ in range(size_1 - size_2):\n",
    "            e2 = np.append(e2, 0.0)\n",
    "            \n",
    "    return [e1, e2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-session",
   "metadata": {},
   "source": [
    "#### custom build and fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excellent-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defined-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "resident-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('processed_Dtrain_4f.csv')\n",
    "\n",
    "titles = []\n",
    "for row in corpus['title']:\n",
    "    titles.append((row))\n",
    "    \n",
    "match_titles = []\n",
    "for row in corpus['match_title']:\n",
    "    match_titles.append((row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regulated-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    \n",
    "    inp_example = InputExample(texts=[titles[i], match_titles[i]], label=1.0)\n",
    "    train_samples.append(inp_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aggressive-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "train_batch_size = 16\n",
    "model_save_path = './sbert_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "threaded-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "structural-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "democratic-ecuador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f61b212c375483a99dd3e4adba0c837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05fa105742d48ae922fe85d617dd4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/19253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4d7641476528>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mwarmup_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           show_progress_bar = True)\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Akshay\\.conda\\envs\\pytorchenv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[0;32m    704\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                         \u001b[0mloss_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-thumb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "framed-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = model.encode(\"Jo Malone London™ Dark Amber & Ginger Lily Scented Home Candle NO COLOR  7 oz—\")\n",
    "emb2 = model.encode(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "documentary-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos similarity:  tensor([[0.9204]])\n"
     ]
    }
   ],
   "source": [
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cos similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-vaccine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
